# AI Agent Policy

This document explains how I use AI assistants as tools in this project and clarifies the roles and responsibilities.

## Why This Matters

Using AI responsibly means being transparent about:

- Where AI is involved
- What decisions AI makes vs. what you decide
- How to learn effectively while using AI assistance

This policy ensures the portfolio demonstrates *your* skills and understanding, not just AI's capabilities.

## AI Assistant Role

The AI assistant (currently Claude Code, but may change) functions as your **engineering and research assistant**. This includes:

### ✅ What I Do

- **Repo & Project Setup** — Creating directory structures, organising files, preparing scaffolds
- **Research Compilation** — Finding sources, organising information, creating research frameworks
- **Evidence Structure** — Helping organise lab work into coherent evidence (screenshots, logs, configs)
- **Code Review Preparation** — Writing scripts and flagging them for Sourcery AI review
- **Blog Research Points** — Providing structured research with citations, explanation, and evidence links
- **Technical Questions** — Answering "how does X work?" or "what's the best approach for Y?"

### ❌ What I Don't Do

- **Make decisions about your lab work** — You decide what to test, how to approach problems, what conclusions to draw
- **Write your blog posts** — You synthesize the AI's research and write the narrative
- **Create actual evidence** — The AI doesn't run your lab or generate fake logs/configs
- **Replace your learning** — The AI explains, but you're expected to understand the work and be able to defend your choices

## Your Role

You are responsible for:

### Learning & Understanding

- Actually performing the lab work (not just reading about it)
- Understanding why each decision was made
- Being able to explain your approach to a peer or interviewer

### Decision-Making

- What to build and test
- How to approach problems
- What conclusions to draw from results
- What to blog about and how to frame it

### Authenticity

- All lab work is yours (your VMs, your screenshots, your configs)
- All blog writing is yours (you synthesize the research and write the narrative)
- You can reference AI assistance in this policy, but the work itself is authentic

### Verification

- Test your own configurations
- Verify AI-provided information (especially citations and technical details)
- Document your reasoning, not just the outcome

## Risks of Using AI

While AI can be a powerful tool, it is essential to be aware of the following risks:

- **Hallucination**: AI may generate incorrect or misleading information that appears plausible.
- **Cognitive Dissonance**: Users may struggle to reconcile AI-generated outputs with their own knowledge or beliefs.
- **Fallacies**: Various logical fallacies can arise from over-reliance on AI, leading to poor decision-making.
- **Erosion of Human Skills**: Continuous dependence on AI for tasks such as analysis and writing can diminish critical thinking and creativity skills.

## Pre-commit Checklist

Before committing any work that relies on human skills, consider the following questions:

1. Have I critically evaluated the AI-generated content for accuracy?
2. Am I confident in my understanding of the subject matter?
3. Have I maintained my analytical skills in this process?
4. Does this work reflect my personal insights and creativity?
5. Have I consulted other sources or experts to validate the information?

This checklist serves as a reminder to prioritize human skills and judgment in all AI-assisted tasks.

## Blog Post Example

**How it works:**

1. You complete a lab task (e.g., write a SIEM rule for PowerShell execution)
2. You ask the AI assistant: "Can you give me research points on SIEM rule tuning?"
3. The assistant provides:
   - **Point 1:** False positives and tuning strategies
     - Evidence: Your actual lab metrics (% false positives before/after tuning)
     - Citation: MITRE ATT&CK documentation on PowerShell abuse
     - Explanation: Why this matters for operational efficiency
   - **Point 2:** Detection quality vs. noise tradeoff
     - Evidence: Your rule logs showing what you caught
     - Citation: Industry best practices
     - Explanation: How you approached this problem

4. **You write** a blog post that:
   - Opens with your problem ("How do I detect PowerShell attacks without overwhelming my team?")
   - Explains your approach ("I decided to tune using precision/recall metrics")
   - Incorporates evidence ("My logs show...") and reasoning ("This matters because...")
   - Reflects on what you learned

The result: A post where **you're the author**, but the AI assistant helped organise your research.

## Transparency in Commits & Documentation

- This policy file exists in the repo — anyone can read it
- Your day-logs and evidence are yours
- When you reference research or tools, cite them (including AI, where relevant)
- Your blog posts represent your understanding and work

## Questions to Ask Yourself

If you're unsure whether something is over-relying on AI:

- **Could I explain this to a technical peer?** If not, dig deeper.
- **Did I do the actual lab work?** If you only read about it, that doesn't count.
- **Did I make the decision?** If the AI decided, that's a sign you should step back.
- **Am I learning?** If you're just copying/pasting, you're not building skills.

---

**Summary:** Use AI assistants to work *smarter*, not to skip the learning. The goal is a portfolio that demonstrates *your* capabilities, with AI as a transparent supporting tool.
